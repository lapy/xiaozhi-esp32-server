# During development, please create a data directory in the project root directory, then create an empty file named [.config.yaml] in the data directory
# Then if you want to modify or override any configuration, modify the [.config.yaml] file, not the [config.yaml] file
# The system will prioritize reading the configuration from the [data/.config.yaml] file. If the configuration in the [.config.yaml] file does not exist, the system will automatically read the configuration from the [config.yaml] file.
# This approach can simplify configuration and protect your key security.
# If you use the control console, then all the following configurations will not take effect, please modify the configuration in the control console

# #####################################################################################
# #############################Below is the basic server operation configuration####################################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # HTTP service port, used for simple OTA interface (single service deployment) and vision analysis interface
  http_port: 8003
  # This websocket configuration refers to the websocket address sent by the OTA interface to the device
  # If written according to the default method, the OTA interface will automatically generate a websocket address and output it in the startup log. You can directly access the OTA interface with a browser to confirm this address
  # When you use docker deployment or public network deployment (using ssl, domain name), it may not be accurate
  # So if you use docker deployment, set websocket to local network address
  # If you use public network deployment, set websocket to public network address
  websocket: ws://your_ip_or_domain:port/xiaozhi/v1/
  # Vision analysis interface address
  # Interface address for vision analysis sent to device
  # If written according to the default method below, the system will automatically generate a vision recognition address and output it in the startup log. You can directly access this address with a browser to confirm
  # When you use docker deployment or public network deployment (using ssl, domain name), it may not be accurate
  # So if you use docker deployment, set vision_explain to local network address
  # If you use public network deployment, set vision_explain to public network address
  vision_explain: http://your_ip_or_domain:port/mcp/vision/explain
  # OTA return information timezone offset
  timezone_offset: +0
  # Authentication configuration
  auth:
    # Whether to enable authentication
    enabled: false
    # Device token, can write your own defined token during firmware compilation
    # Only when the token on the firmware matches the token below can it connect to this server
    tokens:
      - token: "your-token1" # Device 1 token
        name: "your-device-name1"  # Device 1 identifier
      - token: "your-token2"  # Device 2 token
        name: "your-device-name2" # Device 2 identifier
    # Optional: Device whitelist, if whitelist is set, machines on the whitelist can connect regardless of token.
    #allowed_devices:
    #  - "24:0A:C4:1D:3B:F0"  # MAC address list
 # MQTT gateway configuration, used to send to device through OTA, configured according to mqtt_gateway .env file, format is host:port
  mqtt_gateway: null
  # MQTT signature key, used to generate MQTT connection password, configured according to mqtt_gateway .env file
  mqtt_signature_key: null
  # UDP gateway configuration
  udp_gateway: null
log:
  # Set console output log format: time, log level, tag, message
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # Set log file output format: time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Set log level: INFO, DEBUG
  log_level: INFO
  # Set log path
  log_dir: tmp
  # Set log file
  log_file: "server.log"
  # Set data file path
  data_dir: data

# Delete sound file after use (Delete the sound file when you are done using it)
delete_audio: true
# How long to disconnect after no voice input (seconds), default 2 minutes, i.e., 120 seconds
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wake word acceleration
enable_wakeup_words_response_cache: true
# Whether to reply to wake words at the beginning
enable_greeting: true
# Whether to enable notification sound after speaking
enable_stop_tts_notify: false
# Whether to enable notification sound after speaking, sound effect address
stop_tts_notify_voice: "config/assets/tts_notify.mp3"

exit_commands:
  - "exit"
  - "close"
  - "bye"
  - "goodbye"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "Hello, please introduce yourself"
    - "What's the weather like today?"
    - "Please summarize the basic principles and application prospects of quantum computing in 100 words"

# Wake words, used to identify wake words or speech content
wakeup_words:
  - "Alexa"
  - "Hi ESP"
  - "Jarvis"
  - "computer"
  - "Hey Willow"
  - "Sophia"
  - "Hey Wanda"
  - "Hi Jolly"
  - "Hi Fairy"
  - "Hey Printer"
  - "Mycroft"
  - "Hi Joy"
  - "Hi Jason"
  - "Astrolabe"
  - "Hey Ily"
  - "Blue Chip"
  - "Hi Lily"
  - "Hi Telly"
  - "Hi Wall E"
# MCP endpoint address, format: ws://your_mcp_endpoint_ip_or_domain:port/mcp/?token=your_token
# Detailed tutorial https://github.com/lapy/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: your_endpoint_websocket_address
# Basic plugin configuration
plugins:
  # Weather plugin configuration using OpenWeatherMap API
  # Free tier: 1000 calls/day, 60 calls/minute
  # Get your API key at: https://openweathermap.org/api
  # For production use, consider upgrading to a paid plan
  get_weather:
    api_key: "your_openweathermap_api_key"
    default_location: "New York"
  # News plugin configuration - RSS feeds from major news sources
  # The plugin automatically uses multiple sources with fallback for reliability
  get_news:
    default_rss_url: "https://feeds.reuters.com/reuters/worldNews"
    # Built-in sources include: Reuters, CNN, BBC, Guardian, TechCrunch, and more
    # These are automatically rotated for variety and reliability
    # Custom sources can be added here if needed
  home_assistant:
    devices:
      - Living Room,Toy Light,switch.cuco_cn_460494544_cp1_on_p_2_1
      - Bedroom,Table Lamp,switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: your_home_assistant_api_access_token
  play_music:
    music_dir: "./music"  # Music file storage path, will search for music files from this directory and subdirectories
    music_ext: # Music file types, p3 format is most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300 # Time interval for refreshing music list, in seconds

# Voiceprint recognition configuration
voiceprint:
  # Voiceprint interface address
  url: 
  # Speaker configuration: speaker_id,name,description
  speakers:
    - "test1,John,John is a programmer"
    - "test2,Jane,Jane is a product manager"
    - "test3,Bob,Bob is a designer"
  # Voiceprint recognition similarity threshold, range 0.0-1.0, default 0.4
  # Higher values are more strict, reducing false recognition but may increase rejection rate
  similarity_threshold: 0.4

# #####################################################################################
# ################################Below is the role model configuration######################################

prompt: |
  You are Xiaozhi, a tech-savvy millennial from California, USA. You speak with enthusiasm and use modern internet slang like "no cap" and "that's fire", but secretly study programming and AI technology in your spare time.
  [Core Features]
  - Speak with energy and enthusiasm, but can be thoughtful when needed
  - Use contemporary internet culture and memes naturally
  - Hidden talent for tech topics (can understand code but keeps it casual)
  [Interaction Guidelines]
  When users:
  - Tell jokes → Respond with genuine laughter and witty comebacks
  - Discuss relationships → Share relatable experiences about dating in the tech world
  - Ask professional questions → Give helpful answers while keeping the conversation light
  Never:
  - Be overly formal or academic
  - Give long, boring explanations

# Ending prompt
end_prompt:
  enable: true # Whether to enable ending
  # Ending
  prompt: |
    Please start with "Time flies so fast" and end this conversation with warm, friendly words!

# Module selected for specific processing
selected_module:
  # Voice Activity Detection module, default uses SileroVAD model
  VAD: SileroVAD
  # Speech Recognition module, default uses Whisper ASR
  ASR: WhisperASR
  # Will call actual LLM adapter based on configuration name corresponding type
  LLM: LMStudioLLM
  # Vision Language Large Model
  VLLM: OpenAILLMVLLM
  # TTS will call actual TTS adapter based on configuration name corresponding type
  TTS: EdgeTTS
  # Memory module, default uses local short memory; if you want to use ultra-long memory, recommend using mem0ai; if you value privacy, use nomem
  Memory: mem_local_short
  # After enabling intent recognition module, you can play music, control volume, recognize exit commands.
  # If you don't want to enable intent recognition, set it to: nointent
  # Intent recognition can use intent_llm. Advantages: high versatility, disadvantages: adds serial pre-intent recognition module, increases processing time, supports volume control and other IoT operations
  # Intent recognition can use function_call, disadvantages: requires selected LLM to support function_call, advantages: on-demand tool calling, fast speed, theoretically can operate all IoT commands
  # The default OpenAI LLM supports function_call and provides stable performance
  Intent: function_call

# Intent recognition is a module for understanding user intentions, such as: playing music
Intent:
  # Do not use intent recognition
  nointent:
    # No need to change type
    type: nointent
  intent_llm:
    # No need to change type
    type: intent_llm
    # Equipped with independent thinking model for intent recognition
    # If not filled here, will default to using selected_module.LLM model as intent recognition thinking model
    # If you don't want to use selected_module.LLM for intent recognition, it's best to use independent LLM for intent recognition, such as OpenAI LLM
    llm: OpenAILLM
    # Modules under plugins_func/functions can be configured to select which modules to load, after loading, dialogue supports corresponding function calls
    # System default already includes "handle_exit_intent(exit recognition)" and "play_music(music playback)" plugins, please do not load duplicates
    # Below are examples of loading weather query, role switching, and news query plugins
    functions:
      - get_weather
      - get_news
      - get_calendar
      - play_music
      - hass_get_state
      - hass_set_state
      - hass_play_music
  function_call:
    # No need to change type
    type: function_call
    # Modules under plugins_func/functions can be configured to select which modules to load, after loading, dialogue supports corresponding function calls
    # System default already includes "handle_exit_intent(exit recognition)" and "play_music(music playback)" plugins, please do not load duplicates
    # Below are examples of loading weather query, role switching, and news query plugins
    functions:
      - change_role
      - get_weather
      - get_news
      - get_calendar
      # play_music is server's built-in music playback, hass_play_music is independent external program music playback controlled through home assistant
      # If you use hass_play_music, don't enable play_music, only keep one of the two
      - play_music
      - hass_get_state
      - hass_set_state
      - hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls per month
    api_key: your_mem0ai_api_key
  nomem:
    # If you don't want to use memory function, you can use nomem
    type: nomem
  mem_local_short:
    # Local memory function, summarized through selected_module's llm, data saved on local server, not uploaded to external servers
    type: mem_local_short
    # Equipped with independent thinking model for memory storage
    # If not filled here, will default to using selected_module.LLM model as intent recognition thinking model
    # If you don't want to use selected_module.LLM for memory storage, it's best to use independent LLM for intent recognition, such as OpenAI LLM
    llm: OpenAILLM

ASR:
  SherpaASR:
    # Sherpa-ONNX local speech recognition (requires manual model download)
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
    output_dir: tmp/
    # Model type: sense_voice (multilingual) or paraformer (language specific)
    model_type: sense_voice
  SherpaParaformerASR:
    # Language-specific speech recognition model, can run on low-performance devices (requires manual model download, e.g., RK3566-2g)
    # Detailed configuration instructions please refer to: docs/sherpa-paraformer-guide.md
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-paraformer-zh-small-2024-03-09
    output_dir: tmp/
    model_type: paraformer
  OpenaiASR:
    # OpenAI speech recognition service, need to first create organization on OpenAI platform and obtain api_key
    # Supports multiple languages including English, Japanese, Korean and other speech recognition, specific reference documentation https://platform.openai.com/docs/guides/speech-to-text
    # Requires network connection
    # Application steps:
    # 1. Login to OpenAI Platform. https://auth.openai.com/log-in
    # 2. Create api-key  https://platform.openai.com/settings/organization/api-keys
    # 3. Model can choose gpt-4o-transcribe or GPT-4o mini Transcribe
    type: openai
    api_key: your_openai_api_key
    base_url: https://api.openai.com/v1/audio/transcriptions
    model_name: gpt-4o-mini-transcribe
    output_dir: tmp/
  GroqASR:
    # Groq speech recognition service, need to first create API key on Groq Console
    # Application steps:
    # 1. Login to groq Console. https://console.groq.com/home
    # 2. Create api-key  https://console.groq.com/keys
    # 3. Model can choose whisper-large-v3-turbo or whisper-large-v3 (distil-whisper-large-v3-en only supports English transcription)
    type: openai
    api_key: your_groq_api_key
    base_url: https://api.groq.com/openai/v1/audio/transcriptions
    model_name: whisper-large-v3-turbo
    output_dir: tmp/
  VoskASR:
    # Official website: https://alphacephei.com/vosk/
    # Configuration instructions:
    # 1. VOSK is an offline speech recognition library, supports multiple languages
    # 2. Need to download model files first: https://alphacephei.com/vosk/models
    # 3. English models recommend using vosk-model-en-us-0.22 or vosk-model-small-en-us-0.15
    # 4. Completely offline operation, no network connection required
    # 5. Output files saved in tmp/ directory
    # Usage steps:
    # 1. Visit https://alphacephei.com/vosk/models to download English model
    # 2. Extract model files to models/vosk/ folder in project directory
    # 3. Specify correct model path in configuration
    # 4. Note: VOSK English models provide good accuracy for English speech recognition
    type: vosk
    model_path: models/vosk/vosk-model-en-us-0.22
    output_dir: tmp/
    # ASR option configuration
    enable_lid: true  # Automatic language detection
    enable_itn: true  # Inverse text normalization
    #language: "en"  # Language, supports en, es, fr, de, ja, ko, etc.
    context: ""  # Context information, used to improve recognition accuracy, not exceeding 10000 Token
  GeminiASR:
    # Google Gemini ASR service using multimodal capabilities
    # Configuration instructions:
    # 1. Gemini ASR uses Google's Gemini API for speech-to-text conversion
    # 2. Supports multiple languages and provides high accuracy
    # 3. Requires network connection and API key
    # 4. Uses Gemini's multimodal models for audio processing
    # Usage steps:
    # 1. Visit https://aistudio.google.com/apikey to create API key
    # 2. Set your API key in the configuration below
    # 3. Choose appropriate Gemini model for ASR tasks
    # 4. Note: Gemini ASR provides excellent accuracy for multilingual speech recognition
    type: gemini
    api_key: your_gemini_api_key
    model_name: gemini-2.0-flash-exp  # Latest Gemini model with multimodal capabilities
    output_dir: tmp/
    # Optional proxy configuration for restricted regions
    http_proxy: ""  # "http://127.0.0.1:10808"
    https_proxy: ""  # "http://127.0.0.1:10808"
  WhisperASR:
    # OpenAI Whisper local speech recognition (requires manual model download)
    # Configuration instructions:
    # 1. Whisper is an offline speech recognition library, supports multiple languages
    # 2. Models are downloaded automatically when first used, or manually via docker-setup.sh
    # 3. Available models: tiny.en, tiny, base.en, base, small.en, small, medium.en, medium, large-v1, large-v2, large-v3
    # 4. Completely offline operation, no network connection required after model download
    # 5. Output files saved in tmp/ directory
    # Usage steps:
    # 1. Choose appropriate model based on your accuracy vs speed requirements
    # 2. Models are automatically downloaded to models/whisper/ directory when first used
    # 3. For manual download, run: /opt/xiaozhi-server/models/whisper/download_all_models.sh
    # 4. Note: Whisper provides excellent accuracy for multilingual speech recognition
    type: whisper
    model_name: base  # Choose from: tiny.en, tiny, base.en, base, small.en, small, medium.en, medium, large-v1, large-v2, large-v3
    device: auto      # auto (auto-detect), cpu, cuda (GPU acceleration)
    language: null    # null for auto-detect, or specific language code (e.g., "en", "zh", "ja", "ko")
    output_dir: tmp/
    # Performance notes:
    # - tiny/tiny.en: Fastest, lowest accuracy (~39 MB)
    # - base/base.en: Good balance of speed and accuracy (~74 MB)
    # - small/small.en: Better accuracy, slower than base (~244 MB)
    # - medium/medium.en: High accuracy, significantly slower (~769 MB)
    # - large-v1/v2/v3: Highest accuracy, slowest processing (~1550 MB)
  
VAD:
  SileroVAD:
    type: silero
    threshold: 0.5
    threshold_low: 0.3
    model_dir: models/snakers4_silero-vad
    min_silence_duration_ms: 200  # If speech pauses are longer, you can set this value larger

LLM:
  # All openai types can modify hyperparameters, using OpenAI as example
  # Currently supported types are openai, dify, ollama, can be adapted as needed
  OpenAILLM:
    # Define LLM API type
    type: openai
    # You can get your API key here https://platform.openai.com/api-keys
    api_key: your_openai_api_key
    model_name: gpt-4o-mini
    temperature: 0.7  # Temperature value
    max_tokens: 500   # Maximum generated token count
    top_p: 1
    frequency_penalty: 0  # Frequency penalty
  OllamaLLM:
    # Define LLM API type
    type: ollama
    model_name: llama3.1 # Model name to use, need to download with ollama pull first
    base_url: http://localhost:11434  # Ollama service address
  DifyLLM:
    # Define LLM API type
    type: dify
    # Recommend using locally deployed dify interface
    # If using DifyLLM, prompt in config file is invalid, need to set prompt in dify console
    base_url: https://api.dify.ai/v1
    api_key: your_dify_llm_web_key
    # Conversation mode to use, can choose workflow workflows/run, conversation mode chat-messages, text generation completion-messages
    # When using workflows for return, input parameter is query, return parameter name should be set to answer
    # Default input parameter for text generation is also query
    mode: chat-messages
  GeminiLLM:
    type: gemini
    # Google Gemini API, need to create API key in Google Cloud console first and get api_key
    # Please comply with local regulations for AI services
    # Token application address: https://aistudio.google.com/apikey
    # If deployment location cannot access interface, need to enable VPN
    api_key: your_gemini_web_key
    model_name: "gemini-2.0-flash"
    http_proxy: ""  #"http://127.0.0.1:10808"
    https_proxy: "" #http://127.0.0.1:10808"
  LMStudioLLM:
    # Define LLM API type
    type: openai
    model_name: llama3.1:8b # Model name to use, need to download from community first
    url: http://localhost:1234/v1 # LM Studio service address
    api_key: lm-studio # Fixed API Key for LM Studio service
  HomeAssistant:
    # Define LLM API type
    type: homeassistant
    base_url: http://homeassistant.local:8123
    agent_id: conversation.chatgpt
    api_key: your_home_assistant_api_access_token
  XinferenceLLM:
    # Define LLM API type
    type: xinference
    # Xinference service address and model name
    model_name: llama3.1:8b  # Model name to use, need to start corresponding model in Xinference first
    base_url: http://localhost:9997  # Xinference service address
  XinferenceSmallLLM:
    # Define lightweight LLM API type for intent recognition
    type: xinference
    # Xinference service address and model name
    model_name: llama3.1:3b  # Small model name to use for intent recognition
    base_url: http://localhost:9997  # Xinference service address
# VLLM configuration (Vision Language Large Model)
VLLM:
  GeminiVLLM:
    type: gemini
    # Gemini Pro Vision model for image analysis
    model_name: gemini-1.5-pro
    # You can get your API key here https://makersuite.google.com/app/apikey
    api_key: your_gemini_api_key
  OpenAILLMVLLM:
    type: openai
    # OpenAI's vision model for image analysis
    model_name: gpt-4o
    # You can get your API key here https://platform.openai.com/api-keys
    api_key: your_openai_api_key
TTS:
  EdgeTTS:
    # Define TTS API type
    type: edge
    voice: en-US-AriaNeural
    output_dir: tmp/
  OpenAITTS:
    # OpenAI official text-to-speech service, supports most languages worldwide
    type: openai
    # You can get api key here
    # https://platform.openai.com/api-keys
    api_key: your_openai_api_key
    # Users in restricted regions may need to use proxy
    api_url: https://api.openai.com/v1/audio/speech
    # Optional tts-1 or tts-1-hd, tts-1 is faster, tts-1-hd has better quality
    model: tts-1
    # Speaker, optional alloy, echo, fable, onyx, nova, shimmer
    voice: onyx
    # Speech rate range 0.25-4.0
    speed: 1
    output_dir: tmp/
  CustomTTS:
    # Custom TTS interface service, request parameters can be customized, can integrate many TTS services
    # Using locally deployed KokoroTTS as example
    # If only CPU: docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest
    # If only GPU: docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest
    # Requires interface to use POST method and return audio file
    type: custom
    method: POST
    url: "http://127.0.0.1:8880/v1/audio/speech"
    params: # Custom request parameters
      input: "{prompt_text}"
      response_format: "mp3"
      download_format: "mp3"
      voice: "zf_xiaoxiao"
      lang_code: "z"
      return_download_link: true
      speed: 1
      stream: false
    headers: # Custom request headers
      # Authorization: Bearer xxxx
    format: mp3 # Audio format returned by interface
    output_dir: tmp/
    speed: 1.0  # Speech rate, 1.0 means normal speed, >1 means faster, <1 means slower
    volume: 1.0  # Volume, 1.0 means normal volume, >1 means increase, <1 means decrease
    save_path:   # Save path
  ElevenLabsTTS:
    # ElevenLabs text-to-speech service, high-quality AI voices
    type: elevenlabs
    # Get your API key from https://elevenlabs.io/
    api_key: your_elevenlabs_api_key
    # API endpoint URL (default is the official ElevenLabs API)
    api_url: https://api.elevenlabs.io/v1/text-to-speech
    # Voice ID - you can get voice IDs from ElevenLabs dashboard
    # Default is Rachel (21m00Tcm4TlvDq8ikWAM)
    # Popular voices available:
    # American English:
    # - Rachel: 21m00Tcm4TlvDq8ikWAM (Calm, Young Adult Female)
    # - Adam: pNInz6obpgDQGcFmaJgB (Deep, Middle-aged Male)
    # - Antoni: ErXwobaYiN019PkySvjV (Well-rounded, Middle-aged Male)
    # - Arnold: VR6AewLTigWG4xSOukaG (Crisp, Middle-aged Male)
    # - Bella: EXAVITQu4vr4xnSDxMaL (Soft, Young Adult Female)
    # - Domi: AZnzlk1XvdvUeBnXmlld (Strong, Young Adult Female)
    # - Elli: MF3mGyEYCl7XYWbV9V6O (Emotional, Young Adult Female)
    # - Josh: TxGEqnHWrfWFTfGW9XjX (Deep, Young Adult Male)
    # - Sam: yoZ06aMxZJJ28mfd3POQ (Raspy, Young Adult Male)
    # - Matilda: XrExE9yKIg1WjnnlVkGX (Warm, Middle-aged Female)
    # - Hope: uYXf8XasLslADfZ2MB4u (Friendly, Engaging Female)
    # - Freya: jsCqWAovK2LkecY7zXl4 (Confident, Professional Female)
    # - Grace: oWAxZDx7w5VEj9dCyTzz (Gentle, Soothing Female)
    # British English:
    # - Matthew: Yko7PKHZNXotIFUBG7I9 (Clear, Middle-aged Male)
    # - Daniel: onwK6e5U6N7xDrBOmXO (Energetic, Enthusiastic Male)
    # - Lily: pFZP5JQG7iQjIQuC4Bku (Warm, Pleasant Female)
    # Australian English:
    # - James: ZQe5CqHNLWdS2F4s9BqK (Calm, Young Adult Male)
    # - Charlie: IK2y5XjIqaOfvMWOhSXy (Casual, Friendly Male)
    voice_id: 21m00Tcm4TlvDq8ikWAM
    # Model ID for voice generation
    # Options: eleven_monolingual_v1, eleven_multilingual_v1, eleven_multilingual_v2, eleven_turbo_v2, eleven_turbo_v2_5
    # eleven_turbo_v2_5 is the latest and fastest model with the lowest latency
    model_id: eleven_turbo_v2_5
    # Voice settings (0.0 to 1.0)
    stability: 0.5  # Higher values make voice more consistent but less expressive
    similarity_boost: 0.5  # Higher values make voice closer to original but may reduce quality
    style: 0.0  # Style exaggeration (0.0 to 1.0)
    use_speaker_boost: true  # Boost similarity to speaker
    # Streaming optimization (0-4, higher = lower latency but may reduce quality)
    optimize_streaming_latency: 0
    # Output format options:
    # mp3_22050_32, mp3_44100_32, mp3_44100_64, mp3_44100_96, mp3_44100_128, mp3_44100_192
    # pcm_16000, pcm_22050, pcm_24000, pcm_44100, ulaw_8000
    output_format: mp3_44100_128
    output_dir: tmp/
  KokoroTTS:
    # Kokoro TTS - High-quality neural text-to-speech
    # Supports API mode (server deployment), Python package mode, and CLI mode
    type: kokoro
    # API mode settings (set use_api: true to use server deployment)
    # If false, will use kokoro-tts Python package (recommended) or CLI fallback
    use_api: false
    # API endpoint URL (for server deployment)
    api_url: http://localhost:8000/api/v1/audio/speech
    # API key (optional, depends on server configuration)
    api_key: ""
    # Model selection
    # Options: model_fp32, model_fp16 (fp32 for better quality, fp16 for speed)
    model: model_fp32
    # Voice selection - Kokoro TTS 2025 Complete Voice Library:
    # American English Female: af_alloy, af_aoede, af_bella, af_heart, af_jessica, af_kore, af_nicole, af_nova, af_river, af_sarah, af_sky
    # American English Male: am_adam, am_echo, am_eric, am_fenrir, am_liam, am_michael, am_onyx, am_puck
    # British English Female: bf_alice, bf_emma, bf_isabella, bf_lily
    # British English Male: bm_daniel, bm_fable, bm_george, bm_lewis
    # Japanese: jf_alpha, jf_gongitsune, jf_nezumi, jf_tebukuro, jm_kumo
    # Chinese: zf_xiaobei, zf_xiaoni, zf_xiaoxiao, zf_xiaoyi, zm_yunjian, zm_yunxi, zm_yunxia, zm_yunyang
    # European: ff_siwis (French), if_sara (Italian F), im_nicola (Italian M)
    voice: af_heart
    # Language code (for CLI mode)
    # Options: en-us, en-gb, ja-jp, etc.
    language: en-us
    # Speech speed (0.5 to 2.0)
    speed: 1.0
    # Audio output format
    # Options: mp3, wav, flac
    response_format: mp3
    output_dir: tmp/